# Automate Data Capture at Scale with Document AI: Challenge Lab

## Task 1. Enable the Cloud Document AI API and copy lab source files.

1. From the Navigation menu (Navigation menu icon), click APIs & services > Library.
2. Search for Cloud Document AI API, then click the Enable button to use the API in your Google Cloud project.
3. Sử dụng Cloud Shell

```
  mkdir ./document-ai-challenge
  gsutil -m cp -r gs://spls/gsp367/* \
    ~/document-ai-challenge/
```

## Một cách khác

1. Mở cloud shell
2. Thực hiện lệnh sau:

```
gcloud services enable documentai.googleapis.com
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable cloudbuild.googleapis.com
```

## Task 2. Create a form processor

[xem guide ở đây](https://www.cloudskillsboost.google/course_templates/674/labs/586881)

### Create a processor

1. In the console, from the Navigation menu (Navigation menu icon), click Document AI > Overview.
2. Click Explore processors.
3. Click Create Processor for Form Parser, which is a type of general processor.
4. Specify the processor name as form-parser and select the region US (United States) from the list.
5. Click Create to create the general form-parser processor.
   This will create the processor and return to the processor details page that will display the processor ID, status, and the prediction endpoint.
6. Make a note of the Processor ID as you will use it with curl to make a POST call to the API in a later task.

## Task 3. Create Google Cloud resources

### Create input, output, and archive Cloud Storage buckets

```
export PROJECT_ID=$(gcloud config get-value core/project)
export LOCATION=
export INPUT_BUCKET_NAME=${PROJECT_ID}-input-invoices
export INPUT_BUCKET_LOCATION=$LOCATION
export OUTPUT_BUCKET_NAME=${PROJECT_ID}-output-invoices
export OUTPUT_BUCKET_LOCATION=$LOCATION
export ARCHIVE_BUCKET_NAME=${PROJECT_ID}-archived-invoices
export ARCHIVE_BUCKET_LOCATION=$LOCATION

gsutil mb -c standard -l ${INPUT_BUCKET_LOCATION} -b on gs://${INPUT_BUCKET_NAME}
gsutil mb -c standard -l ${OUTPUT_BUCKET_LOCATION} -b on gs://${OUTPUT_BUCKET_NAME}
gsutil mb -c standard -l ${ARCHIVE_BUCKET_LOCATION} -b on gs://${ARCHIVE_BUCKET_NAME}
```

### Create a BigQuery dataset and tables

```
bq --location="US" mk  -d \
    --description "Form Parser Results" \
    ${PROJECT_ID}:invoice_parser_results

cd ~/document-ai-challenge/scripts/table-schema/

bq mk --table \
  invoice_parser_results.doc_ai_extracted_entities \
  doc_ai_extracted_entities.json
```

## Task 4. Deploy the document processing Cloud Run functions

1. Navigate to `scripts` directory:

```
cd ~/document-ai-challenge/scripts
```

2. Assign the Artifact Registry Reader role to the Compute Engine service account:

```
PROJECT_ID=$(gcloud config get-value project)
PROJECT_NUMBER=$(gcloud projects list --filter="project_id:$PROJECT_ID" --format='value(project_number)')

SERVICE_ACCOUNT=$(gcloud storage service-agent --project=$PROJECT_ID)

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member serviceAccount:$SERVICE_ACCOUNT \
  --role roles/pubsub.publisher
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:service-$PROJECT_NUMBER@gs-project-accounts.iam.gserviceaccount.com" \
  --role="roles/iam.serviceAccountTokenCreator"
```

3. Deploy the Cloud Run functions:

```
# doi lai region
export CLOUD_FUNCTION_LOCATION=europe-west1
gcloud functions deploy process-invoices \
  --gen2 \
  --region=${CLOUD_FUNCTION_LOCATION} \
  --entry-point=process_invoice \
  --runtime=python39 \
  --service-account=${PROJECT_ID}@appspot.gserviceaccount.com \
  --source=cloud-functions/process-invoices \
  --timeout=400 \
  --env-vars-file=cloud-functions/process-invoices/.env.yaml \
  --trigger-resource=gs://${PROJECT_ID}-input-invoices \
  --trigger-event=google.storage.object.finalize\
  --service-account $PROJECT_NUMBER-compute@developer.gserviceaccount.com \
  --allow-unauthenticated
```

Cần chỉnh lại `main.py`

```
import base64
import re
import os
import json
from datetime import datetime
from google.cloud import bigquery
from google.cloud import documentai_v1beta3 as documentai
from google.cloud import storage

# Reading environment variables
gcs_output_uri_prefix = "processed"
project_id = "qwiklabs-gcp-03-4d5118371610"
location = "us"
processor_id = "f5a306427c53ae0"
timeout = 300

# Setting variables
gcs_output_uri = f"gs://{project_id}-output-invoices"
gcs_archive_bucket_name = f"{project_id}-archived-invoices"
destination_uri = f"{gcs_output_uri}/{gcs_output_uri_prefix}/"
name = f"projects/{project_id}/locations/{location}/processors/{processor_id}"
dataset_name = 'invoice_parser_results'
table_name = 'doc_ai_extracted_entities'

# Create a dict to create the schema and to avoid BigQuery load job fails due to inknown fields
bq_schema={
    "input_file_name":"STRING",
    "date":"STRING",
    "waybill_number":"STRING",
    "phone_number":"STRING",
    "total_amount":"STRING"
}
bq_load_schema=[]
for key,value in bq_schema.items():
    bq_load_schema.append(bigquery.SchemaField(key,value))

docai_client = documentai.DocumentProcessorServiceClient()
storage_client = storage.Client()
bq_client = bigquery.Client()

def write_to_bq(dataset_name, table_name, entities_extracted_dict):

    dataset_ref = bq_client.dataset(dataset_name)
    table_ref = dataset_ref.table(table_name)

    test_dict=entities_extracted_dict.copy()
    for key,value in test_dict.items():
      if key not in bq_schema:
          print ('Deleting key:' + key)
          del entities_extracted_dict[key]

    row_to_insert =[]
    row_to_insert.append(entities_extracted_dict)

    json_data = json.dumps(row_to_insert, sort_keys=False)
    #Convert to a JSON Object
    json_object = json.loads(json_data)

    job_config = bigquery.LoadJobConfig(schema=bq_load_schema)
    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON

    job = bq_client.load_table_from_json(json_object, table_ref, job_config=job_config)
    error = job.result()  # Waits for table load to complete.
    print(error)

def get_text(doc_element: dict, document: dict):
    # Document AI identifies form fields by their offsets in document text. This function converts offsets to text snippets.
    response = ''
    # If a text segment spans several lines, it will be stored in different text segments.
    for segment in doc_element.text_anchor.text_segments:
        start_index = (
            int(segment.start_index)
            if segment in doc_element.text_anchor.text_segments
            else 0
        )
        end_index = int(segment.end_index)
        response += document.text[start_index:end_index]
    return response

def process_invoice(event):
    content_type = event.headers["content-type"]
    if content_type == "application/json":
        data = event.get_json(silent=True)
    else:
        return {"ok": "false"}, 400

    print(f"thuvh: {project_id}")

    gcs_input_uri = 'gs://' + data['bucket'] + '/' + data['name']
    print('Printing the contentType: ' + data['contentType'])

    if(data['contentType'] == 'image/gif' or data['contentType'] == 'application/pdf' or data['contentType'] == 'image/tiff' ):
        input_config = documentai.types.document_processor_service.BatchProcessRequest.BatchInputConfig(gcs_source=gcs_input_uri, mime_type=data['contentType'])
        # Where to write results
        output_config = documentai.types.document_processor_service.BatchProcessRequest.BatchOutputConfig(gcs_destination=destination_uri)

        request = documentai.types.document_processor_service.BatchProcessRequest(
            name=name,
            input_configs=[input_config],
            output_config=output_config,
        )

        operation = docai_client.batch_process_documents(request)

        # Wait for the operation to finish
        operation.result(timeout=timeout)

        match = re.match(r"gs://([^/]+)/(.+)", destination_uri)
        output_bucket = match.group(1)
        prefix = match.group(2)

        #Get a pointer to the GCS bucket where the output will be placed
        bucket = storage_client.get_bucket(output_bucket)
        blob_list = list(bucket.list_blobs(prefix=prefix))
        print('Output files:')

        for i, blob in enumerate(blob_list):
            # Download the contents of this blob as a bytes object.
            if '.json' not in blob.name:
                print('blob name ' + blob.name)
                print(f"skipping non-supported file type {blob.name}")
            else:
                #Setting the output file name based on the input file name
                print('Fetching from ' + blob.name)
                start = blob.name.rfind("/") + 1
                end = blob.name.rfind(".") + 1
                input_filename = blob.name[start:end:] + 'gif'
                print('input_filename ' + input_filename)

                # Getting ready to read the output of the parsed document - setting up "document"
                blob_as_bytes = blob.download_as_bytes()
                document = documentai.types.Document.from_json(blob_as_bytes)

                #Reading all entities into a dictionary to write into a BQ table
                entities_extracted_dict = {}
                entities_extracted_dict['input_file_name'] = input_filename
                for page in document.pages:
                    for form_field in page.form_fields:
                        field_name = get_text(form_field.field_name,document)
                        field_value = get_text(form_field.field_value,document)
                        if 'date' in field_name.strip().lower():
                            entities_extracted_dict['date'] = field_value
                        if 'waybill' in field_name.strip().lower():
                            entities_extracted_dict['waybill_number'] = field_value
                        if 'phone' in field_name.strip().lower():
                            entities_extracted_dict['phone_number'] = field_value
                        if 'total' in field_name.strip().lower():
                            entities_extracted_dict['total_amount'] = field_value

                print(entities_extracted_dict)
                print('Writing to BQ')
                #Write the entities to BQ
                write_to_bq(dataset_name, table_name, entities_extracted_dict)

        #print(blobs)
        #Deleting the intermediate files created by the Doc AI Parser
        blobs = bucket.list_blobs(prefix=gcs_output_uri_prefix)
        for blob in blobs:
            blob.delete()
        #Copy input file to archive bucket
        source_bucket = storage_client.bucket(data['bucket'])
        source_blob = source_bucket.blob(data['name'])
        destination_bucket = storage_client.bucket(gcs_archive_bucket_name)
        blob_copy = source_bucket.copy_blob(source_blob, destination_bucket, data['name'])
        #delete from the input folder
        source_blob.delete()
    else:
        print('Cannot parse the file type')
    return {"ok": "true"}, 200


if __name__ == '__main__':
    print('Calling from main')
    testEvent={"bucket":project_id+"-input-invoices", "contentType": "application/pdf", "name":"invoice2.pdf"}
    testContext='test'
    process_invoice(testEvent,testContext)

```

**Note**: có thể không ăn `.env.yaml` đổi thành `.env` hoặc fix thẳng trong code

4. Chỉnh sửa các biến môi trường phù hợp: tham khảo [task 6](https://www.cloudskillsboost.google/course_templates/674/labs/586883)

## Task 5. Test and validate the end-to-end solution

```
gsutil -m cp -r ~/document-ai-challenge/invoices/* gs://$INPUT_BUCKET_NAME
```
